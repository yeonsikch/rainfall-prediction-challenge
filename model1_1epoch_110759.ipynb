{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import netCDF4\n",
    "import os, glob\n",
    "import cv2\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from simpledbf import Dbf5\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEED 고정 및 Params 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 21\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE=0.001\n",
    "device= 'cuda:0'\n",
    "\n",
    "eps= 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpw = glob.glob('/data2/yeonsik/rain/TPW/*/*/*.nc')\n",
    "rr = glob.glob('/data2/yeonsik/rain/RR/*/*.nc')\n",
    "gpm = glob.glob('./train/input_nasa_gpm/*/*.tif')\n",
    "gpm_test = glob.glob('./test/input_nasa_gpm/*.tif')\n",
    "gpm += gpm_test\n",
    "tpw.sort()\n",
    "rr.sort()\n",
    "gpm.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_table = pd.DataFrame({\n",
    "    'date' : list(map(lambda x: datetime(int(x[-15:-11]), int(x[-11:-9]), int(x[-9:-7]), int(x[-7:-5])), rr)),\n",
    "    'rr' : rr\n",
    "})\n",
    "\n",
    "tpw_table = pd.DataFrame({\n",
    "    'date' : list(map(lambda x: datetime(int(x[-15:-11]), int(x[-11:-9]), int(x[-9:-7]), int(x[-7:-5])), tpw)),\n",
    "    'tpw' : tpw\n",
    "})\n",
    "\n",
    "\n",
    "gpm_h = []\n",
    "gpm_time = []\n",
    "for i in range(0,len(gpm),2):\n",
    "    gpm_h.append([gpm[i], gpm[i+1]])\n",
    "    gpm_time.append(datetime(int(gpm[i+1][-44:-40]), int(gpm[i+1][-40:-38]), int(gpm[i+1][-38:-36]), int(gpm[i+1][-26:-24]), int(gpm[i+1][-24:-22])) + timedelta(minutes=1) + timedelta(hours=9))\n",
    "\n",
    "gpm_table = pd.DataFrame({\n",
    "    'date' : gpm_time,\n",
    "    'gpm' : gpm_h\n",
    "})\n",
    "\n",
    "tmp = pd.merge(rr_table, tpw_table, on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = pd.read_csv('./train/train_output_seomjingang.csv')\n",
    "test_output = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "del train_output['비고']\n",
    "del test_output['비고']\n",
    "train_output.columns = ['code', 'region', 'date', 'y', 'cp']\n",
    "test_output.columns = ['code', 'region', 'date', 'y']\n",
    "train_output['date'] = list(map(lambda x : datetime.strptime(x, '%Y-%m-%d %H'), train_output['date']))\n",
    "test_output['date'] = list(map(lambda x : datetime.strptime(x, '%Y-%m-%d %H'), test_output['date']))\n",
    "\n",
    "code = list(set(train_output['code']))\n",
    "code.sort()\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "code = np.array(code).reshape(-1,1)\n",
    "one_hot_code = one_hot_encoder.fit_transform(code)\n",
    "one_hot_code = one_hot_code.toarray()\n",
    "code_to_one = {k:torch.tensor(v) for k, v in zip(one_hot_encoder.categories_[0], list(one_hot_code))}\n",
    "\n",
    "train_output['one_hot_code'] = [code_to_one[x] for x in train_output['code']]\n",
    "test_output['one_hot_code'] = [code_to_one[x] for x in test_output['code']]\n",
    "\n",
    "total_table = pd.merge(tmp, gpm_table, on='date', how='inner')\n",
    "total_table = total_table.sort_values(by='date')\n",
    "total_table = pd.merge(total_table, train_output, on='date')\n",
    "\n",
    "total_table_ = pd.merge(tmp, gpm_table, on='date', how='inner')\n",
    "total_table_ = total_table_.sort_values(by='date')\n",
    "test_table = pd.merge(total_table_, test_output, on='date')\n",
    "test_table = test_table.sort_values(by='date')\n",
    "\n",
    "total_table['year'] = total_table['date'].dt.year\n",
    "total_table['month'] = total_table['date'].dt.month\n",
    "test_table['year'] = test_table['date'].dt.year\n",
    "test_table['month'] = test_table['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "del test_output['비고']\n",
    "test_output.columns = ['code', 'region', 'date', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train & valid')\n",
    "print('years :', set(total_table['year']))\n",
    "print('months :', set(total_table['month']))\n",
    "print()\n",
    "print('test')\n",
    "print('years :', set(test_table['year']))\n",
    "print('months :', set(test_table['month']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid_mask는 지역별로 1달씩 간격줘서 골고루 해도 괜찮을 것 같음. (나중에 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mask = (total_table['year']==2020) & (total_table['month']==11) |\\\n",
    "    (total_table['year']==2021) & (total_table['month']==5)\n",
    "train_mask = valid_mask==0\n",
    "\n",
    "train_table = total_table[train_mask]\n",
    "valid_table = total_table[valid_mask]\n",
    "\n",
    "print('train :', train_table.shape)\n",
    "print('valid :', valid_table.shape)\n",
    "print('test :', test_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TPW 개수 :', len(tpw))\n",
    "print('RR 개수 :', len(rr))\n",
    "print('GPM 개수 :', len(gpm_h))\n",
    "print('전체 개수 :', len(total_table))\n",
    "print('결측치 개수 :', total_table.isna().sum().sum())\n",
    "print('사용 데이터 개수 :', len(total_table))\n",
    "print('학습용 데이터셋 :', len(train_table))\n",
    "print('검증용 데이터셋 :', len(valid_table))\n",
    "print('예측 데이터셋 :', len(test_table))\n",
    "print()\n",
    "print('테이블 열 목록')\n",
    "print(total_table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 섬진강 지역 Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geometric information\n",
    "## Cheonlian 2A Image over all Korean Peninsula - 한반도 전역 천리안 이미지 지리정보\n",
    "cheonlian_geo_files = glob.glob('./geometry/input-cheonlian-2A/*.dbf')\n",
    "cheonlian_dbf = Dbf5(cheonlian_geo_files[0]).to_dataframe()\n",
    "cheonlian_dbf['id'] = cheonlian_dbf['id'].map(lambda x: x-1)\n",
    "## Seomjingang - 섬진강 유역의 관측소 지리정보\n",
    "seomjingang_geo_files = glob.glob('./geometry/output-seomjingang/*.dbf')\n",
    "seomjingang_dbf = Dbf5(seomjingang_geo_files[0]).to_dataframe()\n",
    "# left upper, right lower coordinate - 좌상단 우하단 좌표\n",
    "left_x, upper_y = seomjingang_dbf['x'].min(), seomjingang_dbf['y'].max()\n",
    "right_x, lower_y = seomjingang_dbf['x'].max(), seomjingang_dbf['y'].min()\n",
    "# get pixel indices of Seomjingang area from Cheonlian images\n",
    "x_padding = 0.5\n",
    "y_padding = 0.1\n",
    "seomjingang_ids = cheonlian_dbf.loc[(cheonlian_dbf['x']>=left_x-x_padding) & (cheonlian_dbf['x']<=right_x+x_padding)\n",
    "                      & (cheonlian_dbf['y']>=lower_y-y_padding) & (cheonlian_dbf['y']<=upper_y+y_padding)]['id']\n",
    "x1 = min(seomjingang_ids)//900\n",
    "x2 = max(seomjingang_ids)//900\n",
    "y1 = min(map(lambda x: x%900, seomjingang_ids))\n",
    "y2 = max(map(lambda x: x%900, seomjingang_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 방법 1. 한 모델로 전체 지점을 예측하는 모델을 만들자.\n",
    "    - 지점 code를 one-hot vector로 만들어 입력\n",
    "\n",
    "# ~학습 방법 2. 각 지점 별로 모델을 만들자.~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나중에 이미지 scaling 고려해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainDataset(Dataset):\n",
    "    def __init__(self, table, split='train'):\n",
    "        super().__init__()\n",
    "        self.table = table\n",
    "        self.split = split\n",
    "        self.tpw = self._get_tpw_image()\n",
    "        self.rr = self._get_rr_image()\n",
    "        self.gpm = self._get_gpm_image()\n",
    "        self.code = list(table['one_hot_code'])\n",
    "        if split != 'test':\n",
    "            self.label = list(table['y'])\n",
    "\n",
    "    def _get_tpw_image(self):\n",
    "        tpws = []\n",
    "        for path in self.table['tpw']:\n",
    "            nc = netCDF4.Dataset(path, mode='r')\n",
    "            image = nc.variables['TPW'][:]\n",
    "            image = torch.tensor(image[x1:x2, y1:y2])\n",
    "            tpws.append(image)\n",
    "        return tpws\n",
    "    \n",
    "    def _get_rr_image(self):\n",
    "        rrs = []\n",
    "        for path in self.table['rr']:\n",
    "            nc = netCDF4.Dataset(path, mode='r')\n",
    "            image = nc.variables['RR'][:]\n",
    "            image = torch.tensor(image[x1:x2, y1:y2])\n",
    "            rrs.append(image)\n",
    "        return rrs\n",
    "\n",
    "    def _get_gpm_image(self):\n",
    "        gpms = []\n",
    "        for gpm_files in self.table['gpm']:\n",
    "            gpm_mini = []\n",
    "            gpm = torch.zeros((13, 11))\n",
    "            for gpm_file in gpm_files: \n",
    "                with rasterio.open(gpm_file) as img :\n",
    "                    gpm += np.reshape(img.read(), (13, 11))\n",
    "                    gpm_mini.append(gpm)\n",
    "            gpms.append(gpm_mini)\n",
    "        return gpms\n",
    "\n",
    "    def get_table(self):\n",
    "        return self.table\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.split in ['train', 'valid']:\n",
    "            return self.tpw[idx], self.rr[idx], self.gpm[idx], self.code[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.tpw[idx], self.rr[idx], self.gpm[idx], self.code[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = RainDataset(train_table)\n",
    "# valid_dataset = RainDataset(valid_table)\n",
    "# torch.save(train_dataset, './train_dataset.pt')\n",
    "# torch.save(valid_dataset, './valid_dataset.pt')\n",
    "# test_dataset = RainDataset(test_table, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('./train_dataset.pt') #train\n",
    "valid_dataset = torch.load('./valid_dataset.pt')\n",
    "test_dataset = torch.load('./test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone Network를 1:1:1로 할지 2:1로 할지 3으로 할지\n",
    "## Resnet을 쓸지, 직접 짤지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = timm.create_model('resnest50d_4s2x40d', pretrained=False, in_chans=2)\n",
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_rr = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "transforms_gpm = transforms.Compose([\n",
    "    transforms.Resize((14, 14)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.net = timm.create_model('resnest50d_4s2x40d', pretrained=False, in_chans=2)\n",
    "        self.net.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gpm_cnn = nn.Sequential(\n",
    "            # 2 * 14 * 14\n",
    "            nn.Conv2d(2, 32, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1),\n",
    "        ) # 9\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(256+36+12, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, tpw_rr, gpm, one_hot_code):\n",
    "        out1 = self.net(tpw_rr)\n",
    "        out2 = self.gpm_cnn(gpm)\n",
    "        out2 = out2.view(out2.size(0), -1)\n",
    "\n",
    "        out = torch.cat([out1,out2,one_hot_code], axis=1)\n",
    "        out = self.linear_layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = nn.DataParallel(model, output_device=1)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-7, last_epoch=-1)\n",
    "criterion = nn.MSELoss().to('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_e, valid_error_e = [], []\n",
    "train_error, valid_error = [], []\n",
    "\n",
    "best_models = []\n",
    "best_score = 1.6\n",
    "\n",
    "EPOCHS = 70\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tpw, rr, gpm, code, label = data\n",
    "        tpw_rr = torch.stack([tpw,rr], axis=1)\n",
    "        gpm = torch.stack(gpm, axis=1)\n",
    "        tpw_rr = transforms_rr(tpw_rr)\n",
    "        gpm = transforms_gpm(gpm)\n",
    "        tpw_rr = tpw_rr.to(device)\n",
    "        gpm = gpm.to(device)\n",
    "        code = code.type(torch.float)\n",
    "        code = code.to(device)\n",
    "        label = label.type(torch.float)\n",
    "        label = label.to('cuda:1')\n",
    "\n",
    "        output = model(tpw_rr, gpm, code).squeeze()\n",
    "        loss = torch.sqrt(criterion(output, label))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_error.append(loss.item())\n",
    "        running_loss += loss.item()*len(label)\n",
    "    train_error_e.append(running_loss/len(train_dataset))\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            tpw, rr, gpm, code, label = data\n",
    "            tpw_rr = torch.stack([tpw,rr], axis=1)\n",
    "            gpm = torch.stack(gpm, axis=1)\n",
    "            tpw_rr = transforms_rr(tpw_rr)\n",
    "            gpm = transforms_gpm(gpm)\n",
    "            tpw_rr = tpw_rr.to(device)\n",
    "            gpm = gpm.to(device)\n",
    "            code = code.type(torch.float)\n",
    "            code = code.to(device)\n",
    "            label = label.type(torch.float)\n",
    "            label = label.to('cuda:1')\n",
    "\n",
    "            output = model(tpw_rr, gpm, code).squeeze()\n",
    "            loss = torch.sqrt(criterion(output, label))\n",
    "            valid_error.append(loss.item()/len(label))\n",
    "            running_loss += loss.item()*len(label)\n",
    "    valid_error_e.append(running_loss/len(valid_dataset))\n",
    "    scheduler.step()\n",
    "    print(f'[Epoch : {e}] Train : {round(train_error_e[-1], 3)} Valid : {round(valid_error_e[-1], 3)}')\n",
    "\n",
    "    if valid_error_e[-1] < best_score:\n",
    "        best_score = valid_error_e[-1]\n",
    "        model_path = f'./model_{e}.pt'\n",
    "        torch.save(model.module.state_dict(), model_path)\n",
    "        best_models.append(model_path)\n",
    "        print(valid_error_e[-1])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                tpw, rr, gpm, code = data\n",
    "                tpw_rr = torch.stack([tpw,rr], axis=1)\n",
    "                gpm = torch.stack(gpm, axis=1)\n",
    "                tpw_rr = transforms_rr(tpw_rr)\n",
    "                gpm = transforms_gpm(gpm)\n",
    "                tpw_rr = tpw_rr.to(device)\n",
    "                gpm = gpm.to(device)\n",
    "                code = code.type(torch.float)\n",
    "                code = code.to(device)\n",
    "\n",
    "                output = model(tpw_rr, gpm, code).squeeze()\n",
    "                total.extend(output.to('cpu'))\n",
    "\n",
    "        total = [x.item() for x in total]\n",
    "        table = test_dataset.table\n",
    "        table['y'] = total\n",
    "        table['date'] = table['date'].astype(int)\n",
    "        result_table = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "        result_table.columns =['code', 'region', 'date', 'y', 'etc']\n",
    "        result_table['관측시간'] = result_table['date']\n",
    "        result_table['date'] = list(map(lambda x : datetime.strptime(x, '%Y-%m-%d %H'), result_table['date']))\n",
    "        result_table['date'] = result_table['date'].astype(int)\n",
    "        output = pd.merge(result_table, table, on=['date','code'], how='left')\n",
    "        output['y_y'] = output['y_y'].fillna(0)\n",
    "        result_table = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "        result_table['시강수량'] = output['y_y']\n",
    "        result_table.to_csv(f'./output_sample_{e}.csv', index=None)\n",
    "        \n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,16))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(train_error_e)\n",
    "plt.plot(valid_error_e)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = './model_4.pt'\n",
    "model=Net()\n",
    "model.load_state_dict(torch.load(m))\n",
    "model = nn.DataParallel(model, output_device=1)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "total = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        tpw, rr, gpm, code = data\n",
    "        tpw_rr = torch.stack([tpw,rr], axis=1)\n",
    "        gpm = torch.stack(gpm, axis=1)\n",
    "        tpw_rr = transforms_rr(tpw_rr)\n",
    "        gpm = transforms_gpm(gpm)\n",
    "        tpw_rr = tpw_rr.to(device)\n",
    "        gpm = gpm.to(device)\n",
    "        code = code.type(torch.float)\n",
    "        code = code.to(device)\n",
    "\n",
    "        output = model(tpw_rr, gpm, code).squeeze()\n",
    "        total.extend(output.to('cpu'))\n",
    "\n",
    "total = [x.item() for x in total]\n",
    "table = test_dataset.table\n",
    "table['y'] = total\n",
    "table['date'] = table['date'].astype(int)\n",
    "result_table = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "result_table.columns =['code', 'region', 'date', 'y', 'etc']\n",
    "result_table['관측시간'] = result_table['date']\n",
    "result_table['date'] = list(map(lambda x : datetime.strptime(x, '%Y-%m-%d %H'), result_table['date']))\n",
    "result_table['date'] = result_table['date'].astype(int)\n",
    "output = pd.merge(result_table, table, on=['date','code'], how='left')\n",
    "output['y_y'] = output['y_y'].fillna(0)\n",
    "result_table = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "result_table['시강수량'] = output['y_y']\n",
    "result_table.to_csv('./output_sample_e4.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = [x.item() for x in total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = test_dataset.table\n",
    "table['y'] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['date'] = table['date'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "result_table.columns =['code', 'region', 'date', 'y', 'etc']\n",
    "result_table['관측시간'] = result_table['date']\n",
    "result_table['date'] = list(map(lambda x : datetime.strptime(x, '%Y-%m-%d %H'), result_table['date']))\n",
    "result_table['date'] = result_table['date'].astype(int)\n",
    "output = pd.merge(result_table, table, on=['date','code'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.read_csv('./test/sample_output_seomjingang.csv')\n",
    "result_table['시강수량'] = output['y_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.to_csv('./output_sample1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b88b55e0e393079319a9eb621c78beded5280cecbac5c97f46fe1054db03a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
